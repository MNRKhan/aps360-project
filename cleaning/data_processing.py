# -*- coding: utf-8 -*-
"""APS360 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QBLg35LVYAsuh7ByoYOtAgPsWCyQfjs9
"""

# Library Dependencies

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torch.nn.functional as F

import os
import shutil
import time
import re
import sys
import skimage.io as io

import random

from PIL import Image

from torchvision import datasets, models, transforms

#constants
#the fraction of an image
#the object must occupy in order to 
#be considered prominent
PROMINENT_PERCENT_THRESHOLD = 0.1

MAX_PROMINENT_NUM = 4

import pycocotools

cocoData = torchvision.datasets.CocoDetection("/content/coco/train","/content/coco/annotations/instances_val2014.json",
                                            transform = transforms.ToTensor())



#returns the img dictionary and corresponding annotation id dictionary 
#containing images of
#the classes of interest
def generateSelImgDict(coco):
  #all categories
  cats = coco.loadCats(coco.getCatIds())
  
  #the categories ids interest
  #they have a sueprcategory of 
  #person, vehicle or animal
  catIds_ = []
  imgIds = []
  annIds = []
  annDict = []
  
  for cat in cats:
    if cat['supercategory'] == 'person' or cat['supercategory']=='animal' or cat['supercategory']=='vehicle':
        catId = cocoData.coco.getCatIds(catNms = cat['name'])
        imgIdVec = (cocoData.coco).getImgIds(catIds=catId)
      
        for item in imgIdVec:
          imgIds.append(item)
          #the ids for the annotation for this image, but only for the category of
          #interest
          #print(cocoData.coco.getAnnIds(imgIds=item, catIds = catId, iscrowd = None))
          annId = cocoData.coco.getAnnIds(imgIds = item, catIds=catId, iscrowd=None)
          annDict.append(cocoData.coco.loadAnns(annId))
  #print(len(imgIds))

  #create dictionary of images of interest
  imgDict = (cocoData.coco).loadImgs(imgIds)
  
  return imgDict,annDict

#create the image and annotation dictionaries from the data
imgDict,annDict = generateSelImgDict()


#extract masked objects
#returns modified image
def extractWithMask(img,mask):
  mask_ = np.array(mask)
  
  #reshape to give 3rd axis for broadcasting
  #to the 3 channels
  mask_ = np.expand_dims(mask_, axis = 2)
  img_ = np.array(img)
  
  modified_image = img_*mask_
  
  return modified_image

#given a single object's mask
#finds the percent of image that
#object occupies
def getPercentMask(obj_mask):
  num_occ= 0.0
  num_occ += (obj_mask != 0).sum()
  num_tot = sum(len(item) for item in obj_mask)
  
  return num_occ/num_tot

#given annotations for object
#creates a mask of only up to 4 prominent objects
#in the image
def genBinaryProminentMask(anns):
  mask = np.zeros(np.array(cocoData.coco.annToMask(anns[0])).shape)
  
  count = 0
  
  #create conglomerate mask over all objects in image
  for i in range(len(anns)):
    #if we get enough prominent objects in the image
    if (count == MAX_PROMINENT_NUM):
      break;
    
    #extract the mask corresponding to the annotation
    obj_mask = cocoData.coco.annToMask(anns[i])
    
    #compute the percentage (fraction) of image occupied
    #by the mask
    percent = getPercentMask(obj_mask)
    
    #only add if the percent is above some threshold 
    if (percent >= PROMINENT_PERCENT_THRESHOLD):
      mask = np.clip(mask + obj_mask, a_min =0, a_max =1)
      count+=1
      
  return mask


#given an img, crops and resizes it
#to be resize_dim by resize_dim
#fixes aspect ratio by padding to square
def crop_resize_square(img, resize_dim=224):
  img = np.array(img)
  #turn img into a PIL img
  im_pil = Image.fromarray(img, 'RGB')
 
  #to make square, choose dimension for padding first
  #side_len = max(height, width, resize_dim)
  
  #maintain aspect ratio and reduce to a dimension of resize_dim
  size = (resize_dim, resize_dim)
  
  im = im_pil.copy()
  im.thumbnail(size,Image.ANTIALIAS)
  w,h = im.size
  
  #create new image with black background
  square_im = Image.new('RGB', (resize_dim, resize_dim), (0,0,0))
  
  #paste in the original image to the square shaped one
  square_im.paste(im, (int((resize_dim-w)/2), int((resize_dim-h)/2)))
  
  return np.array(square_im)


#given the img and annotation dictionaries
#removes and cleans the data
#so that only those images with
#prominent objects remain

#returns the cleaned images, and their masks
#as targets in a tuple called data
#
def data_parse_prominent(imgDict, annDict, size):
  
  data = []
  
  count = 0
  
  #randomly shuffle indices to 
  #acces different images
  ind = np.arange(len(imgDict))
  random.shuffle(ind)
  
  for i in ind:
    
    #stop when size is reached
    if(count == size):
      break
      
    #generate prominent mask for object:
    mask = genBinaryProminentMask(annDict[i])
    
    #if entire mask is 0
    #there were no prominent objects in the mask
    #we only wish to add prominent objects
    if np.sum(mask)!=0:
      imgInf = imgDict[i]
      img = io.imread(imgInf['coco_url'])
      img = crop_resize_square(img)
      data.append((img, mask))
      count+=1
      
  if (count != size):
    print("Not enough data for size requested")
    
  return data

